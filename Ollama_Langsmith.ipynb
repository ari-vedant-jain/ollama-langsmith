{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating QA System Correctness on Ollama provided LLMs using Langsmith\n",
    "\n",
    "Evaluating a question and answer systems can help you improve its system design as well as the prompt and model quality. We tend to improve what we can measure, so checking for accuracy is a top priority. One challenge for measuring accuracy is that the response are unstructured text. A Q&A system can generate lengthy responses, making traditional metrics like BLEU or ROUGE unreliable. For this scenario, using a well-labeled dataset and llm-assisted evaluators can help you grade your system's response quality. This complements human review and other measurements you might have already implemented.\n",
    "\n",
    "In this walkthrough, we will use LangSmith to check the correctness of a Q&A system against an example dataset. The main steps are:\n",
    "\n",
    "1. Run 2 LLMs locally\n",
    "2. Create a dataset of questions and answers.\n",
    "3. Define your question and answering system.\n",
    "4. Run evaluation using LangSmith.\n",
    "5. Iterate to improve the system.\n",
    "\n",
    "###### Inspired by this [documentation](https://docs.smith.langchain.com/old/cookbook/testing-examples/qa-correctness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "\n",
    "1. In order to install Ollama, see this [link](https://github.com/ollama/ollama?tab=readme-ov-file). \n",
    "2. Full Ollama API docs can be found [here](https://github.com/ollama/ollama/blob/main/docs/api.md).\n",
    "3. Get started guide with [Langsmith](https://docs.smith.langchain.com/)\n",
    "\n",
    "We will use some basic prompting techniques and simple queries to evaluate our LLM. \n",
    "\n",
    "You can further tune the prompts and model hyperparameters. Here is a full list of hyperparameters for Llama3 from Ollama\n",
    "```\n",
    "curl http://localhost:11434/api/generate -d '{\n",
    "  \"model\": \"llama3\",\n",
    "  \"prompt\": \"Why is the sky blue?\",\n",
    "  \"stream\": false,\n",
    "  \"options\": {\n",
    "    \"num_keep\": 5,\n",
    "    \"seed\": 42,\n",
    "    \"num_predict\": 100,\n",
    "    \"top_k\": 20,\n",
    "    \"top_p\": 0.9,\n",
    "    \"tfs_z\": 0.5,\n",
    "    \"typical_p\": 0.7,\n",
    "    \"repeat_last_n\": 33,\n",
    "    \"temperature\": 0.8,\n",
    "    \"repeat_penalty\": 1.2,\n",
    "    \"presence_penalty\": 1.5,\n",
    "    \"frequency_penalty\": 1.0,\n",
    "    \"mirostat\": 1,\n",
    "    \"mirostat_tau\": 0.8,\n",
    "    \"mirostat_eta\": 0.6,\n",
    "    \"penalize_newline\": true,\n",
    "    \"stop\": [\"\\n\", \"user:\"],\n",
    "    \"numa\": false,\n",
    "    \"num_ctx\": 1024,\n",
    "    \"num_batch\": 2,\n",
    "    \"num_gpu\": 1,\n",
    "    \"main_gpu\": 0,\n",
    "    \"low_vram\": false,\n",
    "    \"f16_kv\": true,\n",
    "    \"vocab_only\": false,\n",
    "    \"use_mmap\": true,\n",
    "    \"use_mlock\": false,\n",
    "    \"num_thread\": 8\n",
    "  }\n",
    "}'\n",
    "```\n",
    "\n",
    "Optionally, you can install Ollama-UI to just play around with a Ollama models backed QA application via this [git repo](https://github.com/ollama-ui/ollama-ui).\n",
    "\n",
    "```\n",
    "git clone https://github.com/ollama-ui/ollama-ui\n",
    "cd ollama-ui\n",
    "make\n",
    "\n",
    "open http://localhost:8000 # in browser\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if Llama3 model is running on local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = 'http://localhost:11434/api/generate'\n",
    "data = {\"model\": \"llama3\", \"prompt\": \"Why is the sky blue?\"}\n",
    "headers = {\n",
    "    \"Accept\": \"application/json\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n",
    "x = requests.post(url, headers = headers, data = json.dumps(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OR \n",
    "#### Test with Single Shot Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain import PromptTemplate # Added\n",
    "\n",
    "llm = Ollama(model=\"llama3\", stop=[\"<|eot_id|>\"]) # Added stop token\n",
    "\n",
    "def get_model_response(user_prompt, system_prompt):\n",
    "    # NOTE: No f string and no whitespace in curly braces\n",
    "    template = \"\"\"\n",
    "        <|begin_of_text|>\n",
    "        <|start_header_id|>system<|end_header_id|>\n",
    "        {system_prompt}\n",
    "        <|eot_id|>\n",
    "        <|start_header_id|>user<|end_header_id|>\n",
    "        {user_prompt}\n",
    "        <|eot_id|>\n",
    "        <|start_header_id|>assistant<|end_header_id|>\n",
    "        \"\"\"\n",
    "\n",
    "    # Added prompt template\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"system_prompt\", \"user_prompt\"],\n",
    "        template=template\n",
    "    )\n",
    "    \n",
    "    # Modified invoking the model\n",
    "    response = llm(prompt.format(system_prompt=system_prompt, user_prompt=user_prompt))\n",
    "    \n",
    "    return response\n",
    "\n",
    "\n",
    "response = get_model_response(\"You are an expert at baking.\", \"Can you give me a recipe of blueberry muffins?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Blueberry muffins, a classic favorite! As an expert in the kitchen, I'd be delighted to share with you my tried-and-true recipe for the most scrumptious blueberry muffins. Here's what you'll need:\\n\\nIngredients:\\n\\n* 2 1/4 cups all-purpose flour\\n* 1 cup granulated sugar\\n* 2 teaspoons baking powder\\n* 1 teaspoon salt\\n* 1/2 cup unsalted butter, melted\\n* 1 large egg\\n* 1 cup plain yogurt (low-fat or nonfat is fine)\\n* 2 teaspoons vanilla extract\\n* 2 cups fresh or frozen blueberries\\n* Confectioners' sugar for topping (optional)\\n\\nInstructions:\\n\\n1. Preheat your oven to 375°F (190°C). Line a 12-cup muffin tin with paper liners.\\n2. In a medium bowl, whisk together flour, sugar, baking powder, and salt.\\n3. In a large bowl, whisk together melted butter, egg, yogurt, and vanilla extract.\\n4. Add the dry ingredients to the wet ingredients and stir until just combined. Do not overmix!\\n5. Gently fold in those lovely blueberries.\\n6. Divide the batter evenly among the muffin cups.\\n7. Bake for 20-25 minutes or until a toothpick inserted into the center of a muffin comes out clean.\\n8. Allow the muffins to cool in the tin for 5 minutes before transferring them to a wire rack to cool completely.\\n\\nTips and Variations:\\n\\n* For an extra boost of flavor, use high-quality vanilla extract or add a teaspoon of lemon zest to the batter.\\n* If using frozen blueberries, thaw them first and pat dry with paper towels to remove excess moisture.\\n* To make mini muffins, simply divide the batter among 12-15 mini muffin cups and bake for 15-20 minutes.\\n* For an added crunch, sprinkle a pinch of granulated sugar on top of each muffin before baking.\\n\\nEnjoy your delicious blueberry muffins!\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you receive a 404 response, run the following command in the terminal.\n",
    "```\n",
    "ollama run llama3\n",
    "```\n",
    "\n",
    "Rerun previous cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangSmith Evaluation of Llama Models\n",
    "\n",
    "For the example below, you will need both Llama2 and Llama3. We will show a grader or evaluator model that measures responses from a test model. \n",
    "We will use Llama2 as the test model and Llama3 as the grader model. \n",
    "You will need to ensure the following: \n",
    "##### a. llama2 is running locally. \n",
    "If not run the following command in terminal:\n",
    "```\n",
    "ollama run llama2\n",
    "```\n",
    "##### b. [Langsmith account and API key ](https://docs.smith.langchain.com/how_to_guides/setup/create_account_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Dataset\n",
    "\n",
    "[Langsmith Documentation](https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API key from secrets.json\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "\n",
    "\n",
    "def get_secrets():\n",
    "    with open('secrets.json') as secrets_file:\n",
    "        secrets = json.load(secrets_file)\n",
    "\n",
    "    return secrets\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    secrets = get_secrets()\n",
    "    os.environ[\"LANGCHAIN_API_KEY\"]  = secrets.get(\"LANGCHAIN_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "# Define dataset: these are your test cases\n",
    "dataset_name = \"QA Example Dataset\"\n",
    "dataset = client.create_dataset(dataset_name)\n",
    "client.create_examples(\n",
    "    inputs=[\n",
    "        {\"question\": \"What is LangChain?\"},\n",
    "        {\"question\": \"What is LangSmith?\"},\n",
    "        {\"question\": \"What is OpenAI?\"},\n",
    "        {\"question\": \"What is Google?\"},\n",
    "        {\"question\": \"What is Mistral?\"},\n",
    "    ],\n",
    "    outputs=[\n",
    "        {\"answer\": \"A framework for building LLM applications\"},\n",
    "        {\"answer\": \"A platform for observing and evaluating LLM applications\"},\n",
    "        {\"answer\": \"A company that creates Large Language Models\"},\n",
    "        {\"answer\": \"A technology company known for search\"},\n",
    "        {\"answer\": \"A company that creates Large Language Models\"},\n",
    "    ],\n",
    "    dataset_id=dataset.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define metrics\n",
    "After creating our dataset, we can now define some metrics to evaluate our responses on. Since we have an expected answer, we can compare to that as part of our evaluation. However, we do not expect our application to output those exact answers, but rather something that is similar. This makes our evaluation a little trickier.\n",
    "\n",
    "In addition to evaluating correctness, let's also make sure our answers are short and concise. This will be a little easier - we can define a simple Python function to measure the length of the response.\n",
    "\n",
    "Let's go ahead and define these two metrics.\n",
    "\n",
    "For the first, we will use an LLM to judge whether the output is correct (with respect to the expected output). This LLM-as-a-judge is relatively common for cases that are too complex to measure with a simple function. We can define our own prompt and LLM to use for evaluation here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from langsmith.evaluation import LangChainStringEvaluator\n",
    "from langchain_community.llms import Ollama\n",
    "from langsmith import traceable\n",
    "\n",
    "_PROMPT_TEMPLATE = \"\"\"You are an expert professor specialized in grading students' answers to questions.\n",
    "You are grading the following question:\n",
    "{query}\n",
    "Here is the real answer:\n",
    "{answer}\n",
    "You are grading the following predicted answer:\n",
    "{result}\n",
    "Respond with CORRECT or INCORRECT:\n",
    "Grade:\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    input_variables=[\"query\", \"answer\", \"result\"], template=_PROMPT_TEMPLATE\n",
    ")\n",
    "\n",
    "eval_llm = Ollama(model=\"llama3\", stop=[\"<|eot_id|>\"], temperature=0.4, top_k=3, top_p=0.9) # Added stop token\n",
    "qa_evaluator = LangChainStringEvaluator(\"qa\", config={\"llm\": eval_llm, \"prompt\": PROMPT})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[OPTIONAL] For evaluating the length of the response, this is a lot easier! We can just define a simple function that checks whether the actual output is less than 2x the length of the expected result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langsmith.schemas import Run, Example\n",
    "\n",
    "# def evaluate_length(run: Run, example: Example) -> dict:\n",
    "#     prediction = run.outputs.get(\"output\") or \"\"\n",
    "#     required = example.outputs.get(\"answer\") or \"\"\n",
    "#     score = int(len(prediction) < 2 * len(required))\n",
    "#     return {\"key\":\"length\", \"score\": score}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Evaluations\n",
    "Great! So now how do we run evaluations? Now that we have a dataset and evaluators, all that we need is our application! We will build a simple application that just has a system message with instructions on how to respond and then passes it to the LLM. We will build this using the OpenAI SDK directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain import PromptTemplate # Added\n",
    "\n",
    "llm = Ollama(model=\"llama2\", stop=[\"<|eot_id|>\"]) # Added stop token\n",
    "\n",
    "def get_model_response(user_prompt, system_prompt):\n",
    "    # NOTE: No f string and no whitespace in curly braces\n",
    "    template = \"\"\"\n",
    "        <|begin_of_text|>\n",
    "        <|start_header_id|>system<|end_header_id|>\n",
    "        {system_prompt}\n",
    "        <|eot_id|>\n",
    "        <|start_header_id|>user<|end_header_id|>\n",
    "        {user_prompt}\n",
    "        <|eot_id|>\n",
    "        <|start_header_id|>assistant<|end_header_id|>\n",
    "        \"\"\"\n",
    "\n",
    "    # Added prompt template\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"system_prompt\", \"user_prompt\"],\n",
    "        template=template\n",
    "    )\n",
    "    \n",
    "    # Modified invoking the model\n",
    "    response = llm(prompt.format(system_prompt=system_prompt, user_prompt=user_prompt))\n",
    "    \n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running this through LangSmith evaluations, we need to define a simple wrapper that maps the input keys from our dataset to the function we want to call, and then also maps the output of the function to the output key we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def langsmith_app(inputs):\n",
    "    output = get_model_response(\"Respond to the users question in a short, concise manner (one short sentence).\", inputs[\"question\"])\n",
    "    return {\"output\": output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'Llama-2-local-correctness-38267b4d' at:\n",
      "https://smith.langchain.com/o/103e639e-1fea-5efb-81b6-6b537ff4132d/datasets/e5a1a05f-e750-4972-ae35-0b707d2be74d/compare?selectedSessions=2d020155-f629-48cf-9b8f-d40d1ff5fa8b\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00fa79dbccb641088f63c92f1b43b44b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "experiment_results = evaluate(\n",
    "    langsmith_app, # Your AI system\n",
    "    data=dataset_name, # The data to predict and grade over\n",
    "    evaluators=[qa_evaluator], # The evaluators to score the results\n",
    "    experiment_prefix=\"Llama-2-local-correctness\", # A prefix for your experiment names to easily identify them\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['run', 'example', 'evaluation_results'])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_results._results[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
