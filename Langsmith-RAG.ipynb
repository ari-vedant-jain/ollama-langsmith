{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using LangChain with Ollama in Python\n",
    "\n",
    "Let's imagine we are studying The Youth by Isaax Assimov. We might have a question about Slim and Red. If you ask llama3 for that info, you may get model hallucinations. \n",
    "\n",
    "This sounds like a typical censored response, but even llama3 gives a mediocre answer:\n",
    "\n",
    "> In Stephen Crane's novel \"The Youth\", Slim and Red are two main characters who have a significant interaction. Here's a brief summary:\n",
    "\n",
    "So let's figure out how we can use **LangChain** with Ollama to ask our question to the actual document, [The Youth](https://www.gutenberg.org/cache/epub/31547/pg31547-images.html) by Isaac Assimov, using Python.\n",
    "\n",
    "Let's start by asking a simple question that we can get an answer to from the **Llama2** model using **Ollama**. First, we need to install the **LangChain** package:\n",
    "\n",
    "`pip install langchain_community`\n",
    "\n",
    "Then we can create a model and ask the question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " I apologize, but there is no story called \"The Youth by Isaac Asimov\" that features a character named Slim or Red. It's possible that you may be thinking of a different author or work. Could you please provide more information or context about the story you are asking about?\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "question = \"Can you summarize the interaction between Slim and Red in the Youth by Isaac Assimov?\"\n",
    "ollama = Ollama(\n",
    "    base_url='http://localhost:11434',\n",
    "    model=\"llama2\"\n",
    ")\n",
    "print(ollama.invoke(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create RAG System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrong answerr. Hallucinating.\n",
    "Now let's load a document to ask questions against. I'll load up the Youth by Isaac Assimov, which you can find at Project Gutenberg. We will need **WebBaseLoader** which is part of **LangChain** and loads text from any webpage. On my machine, I also needed to install **bs4** to get that to work, so run `pip install bs4`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://www.gutenberg.org/cache/epub/31547/pg31547-images.html\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Llama2 context size is 4000 tokens, which means the full document won't fit into the context for the model. So we need to split it up into smaller pieces.\n",
    "\n",
    "It's split up, but we have to find the relevant splits and then submit those to the model. We can do this by creating embeddings and storing them in a vector database. We can use Ollama directly to instantiate an embedding model. We will use ChromaDB in this example for a vector database which is provided by Langchain for free. \n",
    "\n",
    "We also need to pull embedding model: `ollama pull nomic-embed-text`\n",
    "\n",
    "You can read more about Ollama supported embedding models [here](https://ollama.com/blog/embedding-models).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "loader = WebBaseLoader(\"https://www.gutenberg.org/cache/epub/31547/pg31547-images.html\")\n",
    "data = loader.load()\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "all_splits = text_splitter.split_documents(data)\n",
    "# create vector embeddings\n",
    "oembed = OllamaEmbeddings(base_url=\"http://localhost:11434\", model=\"nomic-embed-text\")\n",
    "\n",
    "# load splits to vector db\n",
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=oembed)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's ask a question from the document. **Can you summarize the interaction between Slim and Red in the Youth by Isaac Assimov?** Neleus is a character in the Odyssey, and the answer can be found in our text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = vectorstore.similarity_search(question)\n",
    "len(docs)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will output the number of matches for chunks of data similar to the search.\n",
    "\n",
    "The next thing is to send the question and the relevant parts of the docs to the model to see if we can get a good answer. But we are stitching two parts of the process together, and that is called a chain. This means we need to define a chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the provided context, here is a summary of the interaction between Slim and Red:\n",
      "\n",
      "Slim comes into the room unexpectedly and starts talking about having something that can get them into the circus. He proposes starting their own circus and becoming the biggest circus-fellows in the world. Red initially agrees to go along with the plan, but then takes it back after realizing that their parents might not approve of their idea. Slim seems disappointed by this turn of events, as he had been excited about the prospect of having a space-ship scout-ship.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "qachain=RetrievalQA.from_chain_type(ollama, retriever=vectorstore.as_retriever())\n",
    "res = qachain.invoke({\"query\": question})\n",
    "print(res['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracing using LangSmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API key from secrets.json\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "\n",
    "\n",
    "def get_secrets():\n",
    "    with open('secrets.json') as secrets_file:\n",
    "        secrets = json.load(secrets_file)\n",
    "\n",
    "    return secrets\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    secrets = get_secrets()\n",
    "    os.environ[\"LANGCHAIN_API_KEY\"]  = secrets.get(\"LANGCHAIN_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "# Define dataset: these are your test cases\n",
    "dataset_name = \"The Youth QA Dataset\"\n",
    "dataset = client.create_dataset(dataset_name)\n",
    "client.create_examples(\n",
    "    inputs=[\n",
    "        {\"question\": \"What is the significance of the title 'Youth'?\"},\n",
    "        {\"question\": \"Describe the relationship between Red and Slim in the story.\"},\n",
    "        {\"question\": \"How does Asimov use the theme of first contact in 'Youth'?\"},\n",
    "        {\"question\": \"What is the twist at the end of the story 'Youth'?\"},\n",
    "        {\"question\": \"What message does Asimov convey about the differences between children and adults?\"},\n",
    "    ],\n",
    "    outputs=[\n",
    "        {\"answer\": \"The title 'Youth' reflects the story's focus on the perspectives and actions of the young characters, Red and Slim. Their innocence and adventurous spirit contrast sharply with the adult world of negotiations and hidden agendas. The title also highlights the theme of perception and misunderstanding, as the boys' innocent misinterpretation of the situation leads to the story's twist.\"},\n",
    "        {\"answer\": \"Red and Slim share a close friendship based on their mutual curiosity and love for adventure. They are typical boys, eager to explore and discover new things. Their relationship is marked by innocence and a sense of wonder, which contrasts with the more serious and cautious interactions of the adults in the story.\"},\n",
    "        {\"answer\": \"Asimov uses the theme of first contact to explore the potential for both cooperation and misunderstanding between different species. The adults are engaged in serious negotiations, unaware that the animals the boys found are actually alien beings. This twist highlights how assumptions can lead to misunderstandings and how the innocence of youth can reveal truths that adults might overlook.\"},\n",
    "        {\"answer\": \"The twist at the end of 'Youth' is that the two animals that Red and Slim have found are actually the offspring of an alien species. This revelation turns the story on its head, as the adults' negotiations and the boys' innocent play are shown to be interconnected in a way that neither group understood.\"},\n",
    "        {\"answer\": \"Asimov conveys that children and adults perceive the world very differently. Children see the world with innocence and curiosity, often leading them to discover truths that adults might miss due to their preconceived notions and serious concerns. The story suggests that a balance of both perspectives can be valuable, and that sometimes, the simplicity of a child's view can uncover profound truths.\"},\n",
    "    ],\n",
    "    dataset_id=dataset.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Craft the Prompt using Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful knowledgeable assistant, trained to answer\"\n",
    "            \" questions about the Youth by Isaac Assimov.\"\n",
    "            \"'Youth' is set in a future where humanity has achieved interstellar travel and encounters alien civilizations.\"\n",
    "            \"\\nThe current time is {time}.\\n\\nRelevant documents will be retrieved in the following messages.\",\n",
    "        ),\n",
    "        (\"system\", \"{context}\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ").partial(time=str(datetime.now()))\n",
    "\n",
    "model = Ollama(\n",
    "    base_url='http://localhost:11434',\n",
    "    model=\"llama2\"\n",
    ")\n",
    "response_generator = prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an Evaluator LLM\n",
    "Here we will use Llama3 to evaulate the reponses of Llama2 on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model = Ollama(\n",
    "    base_url='http://localhost:11434',\n",
    "    model=\"llama2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finally, assemble the full chain!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The full chain looks like the following\n",
    "from operator import itemgetter\n",
    "\n",
    "chain = (\n",
    "    # The runnable map here routes the original inputs to a context and a question dictionary to pass to the response generator\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\")\n",
    "        | retriever\n",
    "        | (lambda docs: \"\\n\".join([doc.page_content for doc in docs])),\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "    }\n",
    "    | response_generator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually comparing the results of chains in the UI is effective, but it can be time consuming. It can be helpful to use automated metrics and AI-assisted feedback to evaluate your component's performance.\n",
    "\n",
    "Below, we will create a custom run evaluator that logs a heuristic evaluation.\n",
    "\n",
    "Heuristic evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.evaluation import EvaluationResult, run_evaluator\n",
    "from langsmith.schemas import Example, Run\n",
    "\n",
    "\n",
    "@run_evaluator\n",
    "def check_not_idk(run: Run, example: Example):\n",
    "    \"\"\"Illustration of a custom evaluator.\"\"\"\n",
    "    agent_response = run.outputs[\"output\"]\n",
    "    if \"don't know\" in agent_response or \"not sure\" in agent_response:\n",
    "        score = 0\n",
    "    else:\n",
    "        score = 1\n",
    "    # You can access the dataset labels in example.outputs[key]\n",
    "    # You can also access the model inputs in run.inputs[key]\n",
    "    return EvaluationResult(\n",
    "        key=\"not_uncertain\",\n",
    "        score=score,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we will configure the evaluation with the custom evaluator from above, as well as some pre-implemented run evaluators that do the following:\n",
    "\n",
    "* Compare results against ground truth labels.\n",
    "* Checks if the response has sufficient amount of detail\n",
    "* Evaluate 'aspects' of the agent's response in a reference-free manner using custom criteria\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.smith import RunEvalConfig\n",
    "\n",
    "# eval_config = RunEvalConfig(\n",
    "#     evaluators=[\"qa\", \"cot_qa\", \"context_qa\", \"labeled_criteria\"],\n",
    "#     eval_llm=eval_model\n",
    "# )\n",
    "\n",
    "\n",
    "from langchain.evaluation import EvaluatorType\n",
    "from langchain.smith import RunEvalConfig\n",
    "\n",
    "evaluation_config = RunEvalConfig(\n",
    "    eval_llm=eval_model,\n",
    "    # Evaluators can either be an evaluator type (e.g., \"qa\", \"criteria\", \"embedding_distance\", etc.) or a configuration for that evaluator\n",
    "    evaluators=[\n",
    "        # Chain of thought question answering evaluator, which grades answers to questions using chain of thought ‘reasoning’.\n",
    "        EvaluatorType.COT_QA,\n",
    "        # Question answering evaluator that incorporates ‘context’ in the response.\n",
    "        EvaluatorType.CONTEXT_QA,\n",
    "        # Grade whether the output satisfies the stated criteria.\n",
    "        RunEvalConfig.LabeledCriteria(\"detail\"),\n",
    "        # The LabeledScoreString evaluator outputs a score on a scale from 1-10.\n",
    "        # You can use default criteria or write our own rubric\n",
    "        RunEvalConfig.LabeledScoreString(\n",
    "            {\n",
    "                \"accuracy\": \"\"\"\n",
    "Score [[1]]: The answer is completely unrelated to the reference.\n",
    "Score [[3]]: The answer has minor relevance but does not align with the reference.\n",
    "Score [[5]]: The answer has moderate relevance but contains inaccuracies.\n",
    "Score [[7]]: The answer aligns with the reference but has minor errors or omissions.\n",
    "Score [[10]]: The answer is completely accurate and aligns perfectly with the reference.\"\"\"\n",
    "            },\n",
    "            normalize_by=10,\n",
    "        ),\n",
    "    ],\n",
    "    custom_evaluators=[check_not_idk],\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the evaluation. This makes predictions over the dataset and then uses a set of evaluators to check the correctness on each data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'youth-dataset-217d9409' at:\n",
      "https://smith.langchain.com/o/103e639e-1fea-5efb-81b6-6b537ff4132d/datasets/fc9382e7-72de-416e-a423-9e372b0ef23b/compare?selectedSessions=328ec4e9-fd05-42ad-b04c-f4b25120c60b\n",
      "\n",
      "View all tests for Dataset The Youth QA Dataset at:\n",
      "https://smith.langchain.com/o/103e639e-1fea-5efb-81b6-6b537ff4132d/datasets/fc9382e7-72de-416e-a423-9e372b0ef23b\n",
      "[------------------------------------------------->] 5/5"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3>Experiment Results:</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feedback.score_string:accuracy</th>\n",
       "      <th>feedback.not_uncertain</th>\n",
       "      <th>error</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>run_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9bb70685-e737-41d4-94f7-dcf5bdbe441e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>69.761311</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.022643</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33.220860</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48.381462</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>68.690345</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>86.683496</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>111.830394</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        feedback.score_string:accuracy  feedback.not_uncertain error  \\\n",
       "count                              5.0                     5.0     0   \n",
       "unique                             NaN                     NaN     0   \n",
       "top                                NaN                     NaN   NaN   \n",
       "freq                               NaN                     NaN   NaN   \n",
       "mean                               0.7                     1.0   NaN   \n",
       "std                                0.0                     0.0   NaN   \n",
       "min                                0.7                     1.0   NaN   \n",
       "25%                                0.7                     1.0   NaN   \n",
       "50%                                0.7                     1.0   NaN   \n",
       "75%                                0.7                     1.0   NaN   \n",
       "max                                0.7                     1.0   NaN   \n",
       "\n",
       "        execution_time                                run_id  \n",
       "count         5.000000                                     5  \n",
       "unique             NaN                                     5  \n",
       "top                NaN  9bb70685-e737-41d4-94f7-dcf5bdbe441e  \n",
       "freq               NaN                                     1  \n",
       "mean         69.761311                                   NaN  \n",
       "std          31.022643                                   NaN  \n",
       "min          33.220860                                   NaN  \n",
       "25%          48.381462                                   NaN  \n",
       "50%          68.690345                                   NaN  \n",
       "75%          86.683496                                   NaN  \n",
       "max         111.830394                                   NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from uuid import uuid4\n",
    "unique_id = uuid4().hex[0:8]\n",
    "\n",
    "chain_results = await client.arun_on_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    llm_or_chain_factory=lambda: chain,\n",
    "    evaluation=evaluation_config,\n",
    "    project_name=f\"youth-dataset-{unique_id}\",\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_results.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
